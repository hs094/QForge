import { ProjectProfile, GeneratorOptions } from '../../types';
import { SelfHealingManager } from '../self-healing';
import YAML from 'yaml';

/**
 * Enhances a CircleCI pipeline with additional features
 * @param pipelineContent Original pipeline content
 * @param projectProfile Project profile
 * @param options Generator options
 * @returns Enhanced pipeline content
 */
export function enhanceCircleCIPipeline(
  pipelineContent: string,
  projectProfile: ProjectProfile,
  options: GeneratorOptions
): string {
  try {
    // Parse YAML content
    let pipeline: any;
    try {
      pipeline = YAML.parse(pipelineContent);
    } catch (error) {
      // If parsing fails, return the original content
      console.warn('Warning: Failed to parse CircleCI YAML. Returning original content.');
      return pipelineContent;
    }

    // If pipeline is not valid, return the original content
    if (!pipeline) {
      return pipelineContent;
    }

    // Add a comment header
    const header = `# CircleCI configuration generated by QForge
# Project languages: ${projectProfile.languages.join(', ')}
# Project frameworks: ${projectProfile.frameworks.join(', ')}
# Generated with options: ${JSON.stringify(options)}
`;

    // Ensure version is set to 2.1
    pipeline.version = '2.1';

    // Add security scanning if requested
    if (options.security) {
      enhanceWithSecurity(pipeline, projectProfile);
    }

    // Add self-healing capabilities if requested
    if (options.selfHealing) {
      enhanceWithSelfHealing(pipeline);
    }

    // Convert back to YAML
    const enhancedContent = YAML.stringify(pipeline);

    // Return with header
    return header + enhancedContent;
  } catch (error) {
    // If any error occurs, return the original content
    console.warn(`Warning: Failed to enhance CircleCI pipeline: ${error instanceof Error ? error.message : String(error)}`);
    return pipelineContent;
  }
}

/**
 * Enhances a pipeline with security scanning
 * @param pipeline Pipeline object
 * @param projectProfile Project profile
 */
function enhanceWithSecurity(pipeline: any, projectProfile: ProjectProfile): void {
  // Add security orbs if not present
  if (!pipeline.orbs) {
    pipeline.orbs = {};
  }

  pipeline.orbs.snyk = 'snyk/snyk@1.4.0';

  // Ensure jobs section exists
  if (!pipeline.jobs) {
    pipeline.jobs = {};
  }

  // Add security scanning job
  pipeline.jobs.security_scan = {
    docker: [
      {
        image: 'cimg/base:current'
      }
    ],
    steps: [
      'checkout',
      {
        run: {
          name: 'Install security scanning tools',
          command: 'echo "Installing security scanning tools..."'
        }
      }
    ]
  };

  // Add language-specific security steps
  if (projectProfile.languages.includes('JavaScript') || projectProfile.languages.includes('TypeScript')) {
    pipeline.jobs.security_scan.steps.push({
      run: {
        name: 'Run npm audit',
        command: 'npm audit --audit-level=high || true'
      }
    });

    pipeline.jobs.security_scan.steps.push({
      snyk: {
        'fail-on-issues': false,
        'monitor-on-build': true
      }
    });
  }

  if (projectProfile.languages.includes('Python')) {
    pipeline.jobs.security_scan.steps.push({
      run: {
        name: 'Run safety check',
        command: 'pip install safety && safety check || true'
      }
    });
  }

  if (projectProfile.languages.includes('Java')) {
    pipeline.jobs.security_scan.steps.push({
      run: {
        name: 'Run OWASP Dependency Check',
        command: 'echo "Running OWASP Dependency Check..."'
      }
    });
  }

  // Add security job to workflow
  if (!pipeline.workflows) {
    pipeline.workflows = {
      version: 2,
      main: {
        jobs: ['security_scan']
      }
    };
  } else if (pipeline.workflows.main && Array.isArray(pipeline.workflows.main.jobs)) {
    // Add security_scan to existing workflow
    if (!pipeline.workflows.main.jobs.includes('security_scan')) {
      pipeline.workflows.main.jobs.push('security_scan');
    }
  } else if (pipeline.workflows.main && typeof pipeline.workflows.main.jobs === 'object') {
    // Add security_scan to existing workflow with job configurations
    if (!pipeline.workflows.main.jobs.some((job: any) =>
      typeof job === 'object' && Object.keys(job)[0] === 'security_scan'
    )) {
      pipeline.workflows.main.jobs.push({
        security_scan: {
          requires: ['build']
        }
      });
    }
  }
}

/**
 * Enhances a pipeline with self-healing capabilities
 * @param pipeline Pipeline object
 */
function enhanceWithSelfHealing(pipeline: any): void {
  // Ensure jobs section exists
  if (!pipeline.jobs) {
    pipeline.jobs = {};
  }

  // Add retry to all jobs
  for (const [jobName, job] of Object.entries(pipeline.jobs)) {
    if (typeof job === 'object' && job !== null) {
      // Add environment variables for diagnostics
      if (!(job as any).environment) {
        (job as any).environment = {};
      }

      // Add steps for environment diagnostics if not present
      if (!Array.isArray((job as any).steps)) {
        (job as any).steps = [];
      }

      // Add step to capture environment info at the beginning
      (job as any).steps.unshift({
        run: {
          name: 'Capture environment info',
          command: `
echo "Running on $(uname -a)"
echo "Available disk space: $(df -h .)"
echo "Available memory: $(free -h 2>/dev/null || vm_stat 2>/dev/null || echo 'Memory info not available')"
mkdir -p ~/logs
          `,
          when: 'always'
        }
      });

      // Add retry for flaky steps
      for (let i = 0; i < (job as any).steps.length; i++) {
        const step = (job as any).steps[i];

        // If step is a run command
        if (typeof step === 'object' && step.run) {
          // Add timeout if not present
          if (!step.run.no_output_timeout) {
            step.run.no_output_timeout = '10m';
          }

          // Add error handling for test and build steps
          if (step.run.name && (
            step.run.name.toLowerCase().includes('test') ||
            step.run.name.toLowerCase().includes('build') ||
            step.run.name.toLowerCase().includes('lint') ||
            step.run.name.toLowerCase().includes('deploy')
          )) {
            // Wrap command with error handling
            const originalCommand = step.run.command;
            step.run.command = `
set +e
${originalCommand}
EXIT_CODE=$?
if [ $EXIT_CODE -ne 0 ]; then
  echo "Command failed with exit code $EXIT_CODE"
  echo "Collecting diagnostic information..."
  # Save logs
  mkdir -p ~/logs
  find . -name "*.log" -exec cp {} ~/logs/ \\; || true
  find . -name "*.out" -exec cp {} ~/logs/ \\; || true
  find . -path "*/target/surefire-reports/*.xml" -exec cp {} ~/logs/ \\; || true
  find . -path "*/build/test-results/*.xml" -exec cp {} ~/logs/ \\; || true
fi
exit $EXIT_CODE
            `;
          }
        }
      }

      // Add step to store logs as artifacts
      (job as any).steps.push({
        store_artifacts: {
          path: '~/logs',
          destination: 'logs'
        }
      });
    }
  }

  // Add a job for analyzing failures
  pipeline.jobs.analyze_failures = {
    docker: [
      {
        image: 'cimg/base:current'
      }
    ],
    steps: [
      'checkout',
      {
        attach_workspace: {
          at: '~/project'
        }
      },
      {
        run: {
          name: 'Install analysis tools',
          command: 'apt-get update && apt-get install -y grep findutils'
        }
      },
      {
        run: {
          name: 'Analyze pipeline failures',
          command: `
echo "Analyzing pipeline failures..."
echo "Checking for common failure patterns:"

# Create logs directory if it doesn't exist
mkdir -p ~/logs

# Check for test failures
if grep -q "FAILED" ~/logs/*.log 2>/dev/null; then
  echo "✗ Test failures detected"
  grep -A 5 -B 5 "FAILED" ~/logs/*.log 2>/dev/null || true
fi

# Check for dependency issues
if grep -q "Could not resolve dependencies" ~/logs/*.log 2>/dev/null; then
  echo "✗ Dependency resolution issues detected"
  grep -A 5 -B 5 "Could not resolve dependencies" ~/logs/*.log 2>/dev/null || true
fi

# Check for memory issues
if grep -q "Out of memory" ~/logs/*.log 2>/dev/null; then
  echo "✗ Memory issues detected"
  grep -A 5 -B 5 "Out of memory" ~/logs/*.log 2>/dev/null || true
fi

# Check for permission issues
if grep -q "Permission denied" ~/logs/*.log 2>/dev/null; then
  echo "✗ Permission issues detected"
  grep -A 5 -B 5 "Permission denied" ~/logs/*.log 2>/dev/null || true
fi

# Check for timeout issues
if grep -q "timed out" ~/logs/*.log 2>/dev/null; then
  echo "✗ Timeout issues detected"
  grep -A 5 -B 5 "timed out" ~/logs/*.log 2>/dev/null || true
fi

echo "Analysis complete"
          `
        }
      },
      {
        run: {
          name: 'Generate self-healing report',
          command: `
cat > self-healing-report.md << 'EOL'
# Self-Healing Report

## Failure Analysis

The workflow failed during execution. Here's an analysis of the potential issues:

### Common Failure Patterns

- **Test Failures**: Check test logs for assertion failures or errors
- **Dependency Issues**: Verify all dependencies are correctly specified and accessible
- **Resource Constraints**: Check if jobs are running out of memory or disk space
- **Permission Problems**: Ensure the workflow has the necessary permissions
- **Configuration Errors**: Validate workflow syntax and configuration

### Recommended Actions

1. **Review Recent Changes**: Look at recent commits that might have introduced the issue
2. **Check Logs**: Examine the detailed logs for specific error messages
3. **Verify Environment**: Ensure all environment variables and secrets are properly set
4. **Test Locally**: Try to reproduce and fix the issue in a local environment
5. **Update Dependencies**: Make sure all dependencies are up to date

## Self-Healing Capabilities

This workflow includes the following self-healing capabilities:

- **Automatic Retries**: Flaky steps will be automatically retried
- **Failure Analysis**: Logs are analyzed to identify common failure patterns
- **Suggested Fixes**: Based on the analysis, specific fixes are suggested
- **Environment Diagnostics**: System information is captured for debugging
- **Log Collection**: Logs are automatically collected for analysis

For more advanced self-healing capabilities, run QForge locally with the --self-healing flag.
EOL

echo "Self-healing report created"
          `
        }
      },
      {
        store_artifacts: {
          path: 'self-healing-report.md',
          destination: 'self-healing-report.md'
        }
      }
    ],
    when: 'on_fail'
  };

  // Add a job for suggesting fixes
  pipeline.jobs.suggest_fixes = {
    docker: [
      {
        image: 'cimg/base:current'
      }
    ],
    steps: [
      'checkout',
      {
        run: {
          name: 'Suggest fixes',
          command: `
echo "Suggested fixes based on analysis:"

# Test failures
echo "1. For test failures:"
echo "   - Add retries for flaky tests"
echo "   - Increase test timeouts"
echo "   - Check for race conditions in tests"
echo "   - Ensure test environment is properly set up"

# Dependency issues
echo "2. For dependency issues:"
echo "   - Update dependencies to compatible versions"
echo "   - Clear dependency caches"
echo "   - Check for network issues when fetching dependencies"
echo "   - Verify package repository access"

# Resource issues
echo "3. For resource constraints:"
echo "   - Increase memory limits"
echo "   - Optimize resource-intensive operations"
echo "   - Split large jobs into smaller ones"
echo "   - Use more powerful resource classes"

# Permission issues
echo "4. For permission issues:"
echo "   - Check CircleCI context permissions"
echo "   - Verify secrets are properly set"
echo "   - Ensure file permissions are correct"
echo "   - Check service account permissions"

# Timeout issues
echo "5. For timeout issues:"
echo "   - Increase no_output_timeout values"
echo "   - Optimize slow operations"
echo "   - Add progress reporting for long-running tasks"
echo "   - Consider breaking up long-running jobs"

echo "For more detailed analysis, run QForge locally with the --self-healing flag"
          `
        }
      }
    ],
    when: 'on_fail',
    requires: ['analyze_failures']
  };

  // Add self-healing jobs to workflow
  if (pipeline.workflows) {
    // Find the main workflow or create one
    let mainWorkflow;
    if (pipeline.workflows.main) {
      mainWorkflow = pipeline.workflows.main;
    } else if (pipeline.workflows.version && Object.keys(pipeline.workflows).length > 1) {
      // Find the first non-version key
      const workflowName = Object.keys(pipeline.workflows).find(key => key !== 'version');
      if (workflowName) {
        mainWorkflow = pipeline.workflows[workflowName];
      }
    }

    if (mainWorkflow) {
      // Add analyze_failures job to the workflow
      if (Array.isArray(mainWorkflow.jobs)) {
        // Add analyze_failures to existing workflow
        if (!mainWorkflow.jobs.includes('analyze_failures')) {
          mainWorkflow.jobs.push('analyze_failures');
        }

        // Add suggest_fixes to existing workflow
        if (!mainWorkflow.jobs.includes('suggest_fixes')) {
          mainWorkflow.jobs.push('suggest_fixes');
        }
      } else if (typeof mainWorkflow.jobs === 'object') {
        // Add analyze_failures to existing workflow with job configurations
        const hasAnalyzeFailures = mainWorkflow.jobs.some((job: any) =>
          typeof job === 'object' && Object.keys(job)[0] === 'analyze_failures'
        );

        if (!hasAnalyzeFailures) {
          mainWorkflow.jobs.push({
            analyze_failures: {
              requires: ['build', 'test'],
              filters: {
                branches: {
                  only: ['main', 'master', 'develop']
                }
              }
            }
          });
        }

        // Add suggest_fixes to existing workflow with job configurations
        const hasSuggestFixes = mainWorkflow.jobs.some((job: any) =>
          typeof job === 'object' && Object.keys(job)[0] === 'suggest_fixes'
        );

        if (!hasSuggestFixes) {
          mainWorkflow.jobs.push({
            suggest_fixes: {
              requires: ['analyze_failures'],
              filters: {
                branches: {
                  only: ['main', 'master', 'develop']
                }
              }
            }
          });
        }
      }
    } else {
      // Create a new main workflow
      pipeline.workflows.main = {
        jobs: ['analyze_failures', 'suggest_fixes']
      };
    }
  } else {
    // Create workflows section
    pipeline.workflows = {
      version: 2,
      main: {
        jobs: ['analyze_failures', 'suggest_fixes']
      }
    };
  }

  // Add orbs for self-healing
  if (!pipeline.orbs) {
    pipeline.orbs = {};
  }

  pipeline.orbs.node = 'circleci/node@5.1.0';
  pipeline.orbs.slack = 'circleci/slack@4.12.5';
}
